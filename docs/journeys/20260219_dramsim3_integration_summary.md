# DRAMsim3 Integration Summary

**Date**: 2026-02-19

## Overview

This document records the integration of [DRAMsim3](https://github.com/umd-memsys/DRAMsim3) as a cycle-accurate memory backend for `hwgc-soft`. DRAMsim3 models bank-level state, row-buffer hits, and queuing—providing higher fidelity than the fixed-latency Naive model.

## Architecture

The integration bridges the Rust event-driven simulator with the C++ DRAMsim3 library through static linking.

- **Build system** (`build.rs`): Uses the `cmake` crate to build DRAMsim3, the `cc` crate to compile a C++ shim, and `bindgen` to generate Rust FFI bindings. Includes fallback logic to locate `LIBCLANG_PATH`.
- **Shim layer** (`src/shim/dramsim3_wrapper.cc`): A thin C++ wrapper that exposes `AddTransaction`, `ClockTick`, `WillAcceptTransaction`, and `IsTransactionDone` to Rust through a C-linkage interface.
- **Rust integration** (`src/simulate/memory.rs`): `DDR4RankDRAMsim3` implements the `DDR4RankModel` trait. It wraps the C++ object in a `Mutex` for thread safety (`DDR4RankModel` requires `Send + Sync`) and uses a `Mutex<LruCache>` to cache speculative latency predictions.
- **Output management**: DRAMsim3 output files (such as `dramsim3epoch.json`, see `Config::InitOtherParams()` in DRAMsim3's `configuration.cc`) are put in `std::env::temp_dir()` to avoid polluting the workspace. These output files are not generated by default (`output_level = 0`).
- **Startup diagnostics**: `CalculateSize()` and `SetAddressMapping()` in DRAMsim3's `configuration.cc` dump the computed DRAM organization (page size, rank count, capacity) and a bit-field layout at startup for visual comparison with the Rust `AddressMapping` bitfield and the code comments. Both functions use `static bool` guards to print only once, even though the config is loaded per-rank.

## Design Decisions & Lessons Learned

### 1. Address Mapping Alignment

**Challenge**: DRAMsim3's address mapping must match the bitwise layout in `hwgc-soft`'s `AddressMapping` struct exactly.

**Mistake**: We set `columns = 128` to match the 7-bit column field directly. However, this ignores the fact that due to `BL = 8`, there are [3 more bits](https://blog.cloudflare.com/ddr4-memory-organization-and-how-it-affects-memory-bandwidth/) in the column address that are implictly set without any bits in the memory address. Setting `columns = 128` produced an incorrect 1 GB rank instead of 8 GB. Since DRAMsim3 calculates the number of ranks using the size arithmetics (`ranks = channel_size / megs_per_rank` in `CalculateSize()`), it models 16 ranks (default `channel_size` is 16384)—far more than the 4 that `hwgc-soft` encodes with its 2-bit rank+DIMM selector (bits 19:18).

**Solution**: We use `columns = 1024` just like other DDR4 `x8` config files in DRAMsim3's `configs` folder (yielding an 8 KB [page size](https://blog.cloudflare.com/ddr4-memory-organization-and-how-it-affects-memory-bandwidth/)) and `channel_size = 32768` (32 GB). DRAMsim3 now correctly computes 32768 / 8192 = 4 ranks per channel, matching the Rust-side configuration: 2 channels × 2 DIMMs × 2 ranks = 8 total ranks, 64 GB system.

**Lesson**: Configuration parameters must reflect the physical DRAM organization (page size, rank count, capacity) instead of blindly following interface bit widths. DRAMsim3 derives its internal structure mathematically from these values. The startup diagnostics now make mismatches visible immediately.

### 2. Transaction Latency Interface

**Challenge**: `hwgc-soft`'s `NMPProcessor` implements multi-cycle works by executing `NMPProcessorWork::Idle` for the first `n-1` cycles until the final cycle (`work.rs`).
The number `n` is by calling the `get_latency` method on the specific work.
However, `DRAMsim3` doesn't support querying the latency of a memory transaction.
Instead, one needs to call `AddTransaction` + `ClockTick` (which mutates internal queues and bank state), and the callback function will notify when a transaction finishes.

**Solution**: `DDR4RankDRAMsim3` runs the full transaction through DRAMsim3 during the first cycle on the processor side, and then caches the result in an LRU cache.
On the subsequent `transaction(&mut self)` call for the same address, it pops the cached latency instead of re-running the simulation.
This implies that the DRAMsim3 tick will run ahead of `NMPProcessor` temporarily.
Another problem is the `dramsim3` instances will not tick when there is no memory transaction (see "Known Limitations").

### 3. Build Robustness

**Challenge**: Linking C++ standard libraries and locating `libclang` for `bindgen` can fail across environments.

**Solution**: `build.rs` probes common `LIBCLANG_PATH` locations (`/usr/lib/llvm-{19,18,14}/lib`) and links `stdc++` explicitly.

### 4. Posted Writes

DRAMsim3 models posted (non-blocking) writes. In `Controller::AddTransaction` (`controller.cc`), a write immediately gets `complete_cycle = clk_ + 1` and is pushed onto `return_queue_`—the write callback fires on the very next `ClockTick`. The actual DRAM writeback happens asynchronously: the controller buffers writes in `write_buffer_` (capacity = `trans_queue_size`, 32 in our config) and drains them later via `ScheduleTransaction`. `WillAcceptTransaction` rejects new writes when the buffer is full, but our single-transaction-at-a-time model never fills it.

Reads take a different path through four stages:

1. **Enqueue** (`Controller::AddTransaction`): The transaction enters `pending_rd_q_` and `read_queue_` with no `complete_cycle` set.
2. **Schedule** (`Controller::ScheduleTransaction` → `TransToCommand`): Each `ClockTick`, the controller picks a transaction from `read_queue_`, converts it to a `READ` command, and pushes it into the per-bank command queue—if the bank will accept it.
3. **Bank state machine** (`BankState::GetReadyCommand` in `bankstate.cc`): The command cannot issue until the target bank is ready. If the bank is `CLOSED`, an `ACTIVATE` must issue first (costing tRCD cycles). If a different row is open, a `PRECHARGE` must precede the activate (costing tRP cycles). Only on a row hit can the `READ` issue directly.
4. **Completion** (`Controller::IssueCommand`): When the `READ` finally issues, the transaction gets `complete_cycle = clk_ + read_delay`, where `read_delay = RL + burst_cycle` (CAS latency plus data burst, computed in `configuration.cc`). The transaction is pushed onto `return_queue_`.

The callback fires in `JedecDRAMSystem::ClockTick` (`dram_system.cc`), which calls `Controller::ReturnDoneTrans(clk)` each tick. `ReturnDoneTrans` scans `return_queue_` for entries where `clk >= complete_cycle` and returns them, triggering `read_callback_`.

This behavior matches real DDR4 controllers. The Naive model does not distinguish reads from writes, applying the same fixed latency to both.

## Verification

We verified the integration using the `scripts/verify_dramsim3.sh` script on the `fop/heapdump.2.binpb.zst` trace.

| Metric | Naive Model | DRAMsim3 Model | DRAMsim3 (all reads) | Note |
| :--- | :--- | :--- | :--- | :--- |
| **Total Cycles** | ~1,881,176 | ~978,557 | ~1,823,631 | Posted writes account for nearly all the cycle reduction |
| **Utilization** | 0.774 | 0.753 | 0.771 | Comparable across all three |
| **Wall-Clock Time** | 1.176s | 0.612s | 1.140s | — |

The "all reads" column forces `is_write = false` in `DDR4RankDRAMsim3::run_transaction`, treating every transaction as a read. The result (1.82M cycles) is nearly identical to the Naive model (1.88M cycles), confirming that posted writes—not row-buffer hits or bank parallelism—drive the 1.9× cycle reduction.

## Known Limitations

- **Virtual addresses**: The memory model expects physical addresses, but heap dumps provide virtual addresses. The upper bits are effectively random, which distorts row-conflict modeling. See the FIXME in `memory.rs`.
- **Per-rank isolation**: Each `DDR4RankDRAMsim3` instance runs an independent DRAMsim3 simulation. The `trans_queue_size` and `cmd_queue_size` are per-channel, so they might be too generous. However, since we only have 1 transaction at a time, it's unlikely to be an issue.
- **Out-of-sync tick**: currently data cache (such as `SetAssociativeCache`) only run `clock_tick` when running `transaction` or `transaction_latency`. This means that when there is no memory transaction, the memory model is not ticking. This means like fewer DRAM refreshes are modelled, since memory transactions run back to back (instead of having some idles in between transactions). A short-term workaround can be to keep track of when a run-ahead transaction will finish, and call `clock_tick` when there's no run-ahead transaction. A long-term solution is to implement the simulator properly to support stalling and potentially out-of-order execution (the existing model is very conservative anyway that doesn't do any work during memory or inter-DIMM communication stalls).

## Usage

Run with DRAMsim3 (config defaults to `configs/DDR4_8Gb_x8_3200.ini`):

```bash
cargo run --release -- <HEAP_DUMP> -o OpenJDK simulate -p 8 -a NMPGC --use-dramsim3
```

Run verification:

```bash
./scripts/verify_dramsim3.sh
```
